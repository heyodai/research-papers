{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_DRIVE_PATH = os.getenv(\"GOOGLE_DRIVE_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>abstract</th>\n",
       "      <th>notes</th>\n",
       "      <th>arxiv_url</th>\n",
       "      <th>em_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [arxiv_id, title, publication_date, abstract, notes, arxiv_url, em_url]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Initial dataframe setup\n",
    "# df = pd.DataFrame(\n",
    "#     columns=[\n",
    "#         \"arxiv_id\",\n",
    "#         \"title\",\n",
    "#         \"publication_date\",\n",
    "#         \"abstract\",\n",
    "#         \"notes\",\n",
    "#         \"arxiv_url\",\n",
    "#         \"em_url\",\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# df.to_excel(f\"{GOOGLE_DRIVE_PATH}/arxiv_papers.xlsx\", index=False)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(url='https://www.emergentmind.com/papers/2404.01810', notes='This is a test note')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get CLI argument of URL\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"url\", help=\"URL of the Emergent Mind paper\")\n",
    "parser.add_argument(\"--notes\", help=\"Notes for the paper\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# # Demo for Jupyter development\n",
    "# args = argparse.Namespace()\n",
    "# args.url = 'https://www.emergentmind.com/papers/2404.01810'\n",
    "# args.notes = 'This is a test note'\n",
    "# args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>abstract</th>\n",
       "      <th>notes</th>\n",
       "      <th>arxiv_url</th>\n",
       "      <th>em_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2403.14562</td>\n",
       "      <td>The Era of Semantic Decoding</td>\n",
       "      <td>2024-03-21</td>\n",
       "      <td>Recent work demonstrated great promise in the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2403.14562</td>\n",
       "      <td>https://www.emergentmind.com/papers/2403.14562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2403.14472</td>\n",
       "      <td>Detoxifying Large Language Models via Knowledg...</td>\n",
       "      <td>2024-03-21</td>\n",
       "      <td>This paper investigates using knowledge editin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2403.14472</td>\n",
       "      <td>https://www.emergentmind.com/papers/2403.14472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2403.16205</td>\n",
       "      <td>Blur2Blur: Blur Conversion for Unsupervised Im...</td>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>This paper presents an innovative framework de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2403.16205</td>\n",
       "      <td>https://www.emergentmind.com/papers/2403.16205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2403.15583</td>\n",
       "      <td>U-ARE-ME: Uncertainty-Aware Rotation Estimatio...</td>\n",
       "      <td>2024-03-22</td>\n",
       "      <td>Camera rotation estimation from a single image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2403.15583</td>\n",
       "      <td>https://www.emergentmind.com/papers/2403.15583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2403.16971</td>\n",
       "      <td>AIOS: LLM Agent Operating System</td>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>The integration and deployment of large langua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2403.16971</td>\n",
       "      <td>https://www.emergentmind.com/papers/2403.16971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     arxiv_id                                              title  \\\n",
       "0  2403.14562                       The Era of Semantic Decoding   \n",
       "1  2403.14472  Detoxifying Large Language Models via Knowledg...   \n",
       "2  2403.16205  Blur2Blur: Blur Conversion for Unsupervised Im...   \n",
       "3  2403.15583  U-ARE-ME: Uncertainty-Aware Rotation Estimatio...   \n",
       "4  2403.16971                   AIOS: LLM Agent Operating System   \n",
       "\n",
       "  publication_date                                           abstract  notes  \\\n",
       "0       2024-03-21  Recent work demonstrated great promise in the ...    NaN   \n",
       "1       2024-03-21  This paper investigates using knowledge editin...    NaN   \n",
       "2       2024-03-24  This paper presents an innovative framework de...    NaN   \n",
       "3       2024-03-22  Camera rotation estimation from a single image...    NaN   \n",
       "4       2024-03-25  The integration and deployment of large langua...    NaN   \n",
       "\n",
       "                          arxiv_url  \\\n",
       "0  https://arxiv.org/abs/2403.14562   \n",
       "1  https://arxiv.org/abs/2403.14472   \n",
       "2  https://arxiv.org/abs/2403.16205   \n",
       "3  https://arxiv.org/abs/2403.15583   \n",
       "4  https://arxiv.org/abs/2403.16971   \n",
       "\n",
       "                                           em_url  \n",
       "0  https://www.emergentmind.com/papers/2403.14562  \n",
       "1  https://www.emergentmind.com/papers/2403.14472  \n",
       "2  https://www.emergentmind.com/papers/2403.16205  \n",
       "3  https://www.emergentmind.com/papers/2403.15583  \n",
       "4  https://www.emergentmind.com/papers/2403.16971  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the excel file\n",
    "df = pd.read_excel(f\"{GOOGLE_DRIVE_PATH}/arxiv_papers.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html class=\"bg-stone-100\">\n",
       "<head>\n",
       "<title>2404.01810 - Surface Reconstruction from Gaussian Splatting via Novel Stereo Views</title>\n",
       "<meta content=\"2404.01810 - Surface Reconstruction from Gaussian Splatting via Novel Stereo Views\" property=\"og:title\"/>\n",
       "<meta content=\"2404.01810 - Surface Reconstruction from Gaussian Splatting via Novel Stereo Views\" property=\"twitter:title\"/>\n",
       "<meta content=\"This paper presents a novel method for surface reconstruction from Gaussian splatting models by synthesizing stereo views and extracting depth information, significantly enhancing accuracy and reducing computational demand.\" name=\"description\"/>\n",
       "<meta content=\"This paper presents a novel method for surface reconstruction from Gaussian splatting models by synthesizing stereo views and extracting depth information, significantly enhancing accuracy and reducing computational demand.\" property=\"og:description\"/>\n",
       "<meta content=\"This paper presents a novel method for surface reconstruction from Gaussian splatting models by synthesizing stereo views and extracting depth information, significantly enhancing accuracy and reducing computational demand.\" property=\"twitter:description\"/>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<meta content=\"https://www.emergentmind.com/papers/2404.01810\" property=\"og:url\"/>\n",
       "<meta content=\"website\" property=\"og:type\"/>\n",
       "<link href=\"https://www.emergentmind.com/papers/2404.01810\" rel=\"canonical\"/>\n",
       "<meta content=\"https://www.emergentmind.com/assets/og_image_thumbnail-a9dad7b02cfe6ca45a3794c8f5ca30dc84a45c2c32c1a9fa0a1ec778738b7780.jpg\" property=\"og:image\"/>\n",
       "<meta content=\"https://www.emergentmind.com/assets/summary_large_image_thumbnail-efa4d0360543e59922e124e877d443c92ce4676c27d9b237478ae703f1a2da4f.jpg\" property=\"twitter:image\"/>\n",
       "<meta content=\"summary_large_image\" property=\"twitter:card\"/>\n",
       "<meta content=\"@emergentmind\" property=\"twitter:site\"/>\n",
       "<link href=\"https://www.emergentmind.com/assets/logo/favicon-e0f26fd04308a9c98635aa08353f11780220c26740b7c8733ca2c4a5bd79040b.png\" rel=\"icon\" sizes=\"any\"/>\n",
       "<link href=\"https://www.emergentmind.com/assets/logo/icon-ca36f21ff6275fcde05a9be02ad97f58b413b83ff4b2e727f91cce8442f65515.svg\" rel=\"icon\" type=\"image/svg+xml\"/>\n",
       "<link href=\"https://www.emergentmind.com/assets/logo/apple-touch-icon-ad1f78e0419265ed63adfe2b5f0dd8f3835497e464535be5da744591c0c096db.png\" rel=\"apple-touch-icon\"/>\n",
       "<link as=\"font\" crossorigin=\"anonymous\" href=\"/assets/sitka_text_regular-9bb187ed85097383d9b0b3b1609fbd27f1020fc962b84e79736e09df931b8de4.woff2\" rel=\"preload\" type=\"font/woff2\"/>\n",
       "<meta content=\"authenticity_token\" name=\"csrf-param\"/>\n",
       "<meta content=\"UfzY5U3JjmbdVKum6E0X_NtSL4xlHcGQjDFpprWME5T6nk_taqlvvOqB0kOYA9liMi1z8mga1ocrz-L8fpkX_Q\" name=\"csrf-token\"/>\n",
       "<script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-MCBE4ZD8TH\"></script>\n",
       "<script>\n",
       "    window.dataLayer = window.dataLayer || [];\n",
       "    function gtag(){dataLayer.push(arguments);}\n",
       "    gtag('js', new Date());\n",
       "\n",
       "    gtag('config', 'G-MCBE4ZD8TH');\n",
       "  </script>\n",
       "<script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script>\n",
       "<script>\n",
       "    (function(h,o,t,j,a,r){\n",
       "        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};\n",
       "        h._hjSettings={hjid:3768487,hjsv:6};\n",
       "        a=o.getElementsByTagName('head')[0];\n",
       "        r=o.createElement('script');r.async=1;\n",
       "        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;\n",
       "        a.appendChild(r);\n",
       "    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');\n",
       "</script>\n",
       "<script async=\"\" src=\"https://cdn.headwayapp.co/widget.js\"></script>\n",
       "<link href=\"https://unpkg.com/tippy.js@6/dist/tippy.css\" rel=\"stylesheet\">\n",
       "<link href=\"/assets/font-face-c978e26d57ba235ae7167711c4b6cbf0e83988b7148a2c449216864d2f3c857c.css\" rel=\"stylesheet\"/>\n",
       "<link href=\"/assets/tailwind-bc79238a81156e9e879d5406c83ab9e865918f306afd9db5b6aa4075cab0c60f.css\" rel=\"stylesheet\"/>\n",
       "<script data-turbo-track=\"reload\" type=\"importmap\">{\n",
       "  \"imports\": {\n",
       "    \"application\": \"/assets/application-196b7076375897afe92ce1cc15be0ea6f6e342740999ed40a8a0935067cc0643.js\",\n",
       "    \"@hotwired/turbo-rails\": \"/assets/turbo.min-569fe252dd55eef2e3cff9a6e83c8b9a2b0e2374a72d15522515e1ff9999ec78.js\",\n",
       "    \"@hotwired/stimulus\": \"/assets/stimulus.min-59f6a188a51873d87a6ae8218ac6e829404b5cacd7f2a8fb7249abfdec5ece6a.js\",\n",
       "    \"@hotwired/stimulus-loading\": \"/assets/stimulus-loading-6024ee603e0509bba59098881b54a52936debca30ff797835b5ec6a4ef77ba37.js\",\n",
       "    \"alpinejs\": \"https://ga.jspm.io/npm:alpinejs@3.10.2/dist/module.esm.js\",\n",
       "    \"reqwest\": \"https://ga.jspm.io/npm:reqwest@2.0.5/reqwest.js\",\n",
       "    \"xhr2\": \"https://ga.jspm.io/npm:xhr2@0.2.1/lib/browser.js\",\n",
       "    \"linkify-string\": \"https://ga.jspm.io/npm:linkify-string@4.0.2/dist/linkify-string.es.js\",\n",
       "    \"linkifyjs\": \"https://ga.jspm.io/npm:linkifyjs@4.0.2/dist/linkify.es.js\",\n",
       "    \"@ryangjchandler/alpine-tooltip\": \"https://ga.jspm.io/npm:@ryangjchandler/alpine-tooltip@1.2.0/dist/module.esm.js\",\n",
       "    \"luxon\": \"/assets/luxon-ae68e346c68113dab2a7d3bdd0836fad456aff6c699b778a37c2c4940d47ccfd.js\",\n",
       "    \"lodash\": \"https://ga.jspm.io/npm:lodash@4.17.21/lodash.js\",\n",
       "    \"js-cookie\": \"https://ga.jspm.io/npm:js-cookie@3.0.5/dist/js.cookie.mjs\",\n",
       "    \"@alpinejs/collapse\": \"https://ga.jspm.io/npm:@alpinejs/collapse@3.13.3/dist/module.esm.js\",\n",
       "    \"mixpanel-browser\": \"https://ga.jspm.io/npm:mixpanel-browser@2.48.1/dist/mixpanel.cjs.js\"\n",
       "  }\n",
       "}</script>\n",
       "<link href=\"/assets/application-196b7076375897afe92ce1cc15be0ea6f6e342740999ed40a8a0935067cc0643.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"/assets/turbo.min-569fe252dd55eef2e3cff9a6e83c8b9a2b0e2374a72d15522515e1ff9999ec78.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"/assets/stimulus.min-59f6a188a51873d87a6ae8218ac6e829404b5cacd7f2a8fb7249abfdec5ece6a.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"/assets/stimulus-loading-6024ee603e0509bba59098881b54a52936debca30ff797835b5ec6a4ef77ba37.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:alpinejs@3.10.2/dist/module.esm.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:reqwest@2.0.5/reqwest.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:xhr2@0.2.1/lib/browser.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:linkify-string@4.0.2/dist/linkify-string.es.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:linkifyjs@4.0.2/dist/linkify.es.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:@ryangjchandler/alpine-tooltip@1.2.0/dist/module.esm.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"/assets/luxon-ae68e346c68113dab2a7d3bdd0836fad456aff6c699b778a37c2c4940d47ccfd.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:lodash@4.17.21/lodash.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:js-cookie@3.0.5/dist/js.cookie.mjs\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:@alpinejs/collapse@3.13.3/dist/module.esm.js\" rel=\"modulepreload\"/>\n",
       "<link href=\"https://ga.jspm.io/npm:mixpanel-browser@2.48.1/dist/mixpanel.cjs.js\" rel=\"modulepreload\"/>\n",
       "<script type=\"module\">import \"application\"</script>\n",
       "</link></head>\n",
       "<body>\n",
       "<div class=\"h-[44px]\">\n",
       "<div class=\"container px-2 sm:px-4 mx-auto flex justify-between h-full\">\n",
       "<!-- begin: Main Navigation -->\n",
       "<nav class=\"flex w-full\">\n",
       "<a class=\"group h-full inline-block flex items-center font-ff-meta\" data-turbo=\"false\" href=\"https://www.emergentmind.com/\">\n",
       "<svg class=\"inline-block mr-2 -translate-y-0.1\" fill=\"none\" height=\"35\" viewbox=\"0 0 35 35\" width=\"35\" xmlns=\"http://www.w3.org/2000/svg\"><g clip-path=\"url(#a)\"><circle cx=\"17.5\" cy=\"17.5\" fill=\"#FADD07\" r=\"17.5\"></circle><path d=\"M32.976 35H17.683c-1.02 0-1.385-1.347-.506-1.863l10.56-6.19a1 1 0 0 1 1.3.256l4.734 6.19A1 1 0 0 1 32.976 35Z\" fill=\"#FADD07\"></path><path d=\"m10.616 9.567.516.516M17.714 7v.73M27.38 16.665h-.73M8.78 16.665h-.731M24.813 9.567l-.516.516M21.774 14.601c.42.725.64 1.549.633 2.386-.003.87-.25 1.72-.71 2.457-.154.246-.298.462-.43.66-.478.716-.805 1.206-.897 2.082h-5.31c-.09-.905-.423-1.396-.94-2.163-.105-.155-.217-.321-.337-.504a4.694 4.694 0 1 1 7.991-4.918ZM19.336 25.429h-3.243\" stroke=\"#231E21\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.75\"></path></g><defs><clippath id=\"a\"><path d=\"M0 0h35v35H0z\" fill=\"#fff\"></path></clippath></defs></svg>\n",
       "<span class=\"hidden text-neutral-500/90 group-hover:text-active-link-color transition duration-100 ease-out sm:block text-lg font-semibold whitespace-nowrap -translate-y-0.1\">Emergent Mind</span>\n",
       "</a>\n",
       "<div class=\"flex w-full items-center justify-between lg:justify-start space-x-2.1\">\n",
       "<div class=\"relative\">\n",
       "<form :class=\"focused &amp;&amp; 'border-gray-400'\" action=\"/search\" class=\"bg-[#E9EAE6] ml-2 sm:ml-4 w-[255px] xs:w-[300px] md:w-[320px] flex list-none border-b border-r border-l border-gray-200 rounded-b-lg items-center relative focus-within:shadow-innerX hover:shadow-innerX h-[44px]\" method=\"get\" x-data=\"{ focused: false }\" x-init=\"() =&gt; { if($refs.search.value !== '') focused = true }\" x-on:click.away=\"if($refs.search.value === '') focused = false\" x-on:keydown.escape=\"focused = false\">\n",
       "<input class=\"search-input !placeholder-red-400\" id=\"search\" name=\"q\" placeholder=\"arXiv id, url, topic, or author\" type=\"text\" value=\"\" x-on:focus=\"focused = true\" x-ref=\"search\"/>\n",
       "<!-- begin: Magnifying Glass -->\n",
       "<button class=\"!cursor-pointer absolute pl-2.5 right-3 cursor-text inline-block\">\n",
       "<svg class=\"text-gray-400 hover:text-gray-500\" fill=\"none\" height=\"17\" viewbox=\"0 0 18 17\" width=\"18\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<g opacity=\"0.8\">\n",
       "<path d=\"M16.7439 16L13.9878 13.2902\" stroke=\"#68685E\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"></path>\n",
       "<path d=\"M8.65813 15.0993C13.2464 15.0993 15.8276 12.5614 15.8276 8.05031C15.8276 3.5392 13.2464 1 8.65813 1C4.06991 1 1.4873 3.5379 1.4873 8.05031C1.4873 12.5627 4.06859 15.0993 8.65813 15.0993Z\" stroke=\"#68685E\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"></path>\n",
       "</g>\n",
       "</svg>\n",
       "</button>\n",
       "<!-- /end: Magnifying Glass -->\n",
       "</form>\n",
       "</div>\n",
       "<div @click.outside=\"open = false\" class=\"h-full flex items-center justify-end grow\" x-data=\"{ open: false }\">\n",
       "<!-- Hamburger Icon -->\n",
       "<button @click=\"open = !open\" class=\"lg:hidden h-full\">\n",
       "<svg class=\"w-6 h-6 text-stone-500\" fill=\"none\" height=\"14\" viewbox=\"-0.5 -0.5 14 14\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"><g id=\"hamburger-menu-1--button-parallel-horizontal-lines-menu-navigation-three-hamburger\"><path d=\"M0.7688571428571428 1.5962142857142858h11.462285714285715\" id=\"Vector 185\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1\"></path><path d=\"M0.7688571428571428 6.239071428571429h11.462285714285715\" id=\"Vector 186\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1\"></path><path d=\"M0.7688571428571428 10.88192857142857h11.462285714285715\" id=\"Vector 187\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1\"></path></g></svg>\n",
       "</button>\n",
       "<!-- Mobile Menu -->\n",
       "<div @click=\"open = false\" class=\"fixed inset-0 z-50 bg-gray-600 bg-opacity-50 sm:hidden\" x-cloak=\"\" x-show=\"open\"></div>\n",
       "<div class=\"mobile-nav fixed inset-y-0 left-0 z-50 w-64 shadow-lg lg:hidden bg-stone-200 p-6\" x-cloak=\"\" x-show=\"open\">\n",
       "<a class=\"mb-3 block\" href=\"/subscribe\" title=\"Subscribe by Email\">\n",
       "<div class=\"flex items-center space-x-1.5 group-hover bg-[#FADD07] hover:bg-[#fbe439] py-2 px-4 rounded-xl\">\n",
       "<svg class=\"text-[#AA5F03]\" fill=\"none\" height=\"15\" viewbox=\"0 0 16 15\" width=\"16\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M1.12369 11.6174C1.25877 12.7055 2.13777 13.565 3.22825 13.6791C4.7372 13.8367 6.30109 14.0033 7.90371 14.0033C9.50633 14.0033 11.0702 13.8367 12.5791 13.6791C13.6696 13.565 14.5486 12.7055 14.6837 11.6174C14.845 10.3183 15.0037 8.97489 15.0037 7.59973C15.0037 6.22456 14.845 4.88111 14.6837 3.58205C14.5486 2.49399 13.6696 1.63445 12.5791 1.52045C11.0702 1.36272 9.50633 1.19617 7.90371 1.19617C6.30109 1.19617 4.73719 1.36272 3.22825 1.52045C2.13777 1.63445 1.25877 2.49399 1.12368 3.58205C0.962399 4.88111 0.803711 6.22456 0.803711 7.59973C0.803711 8.97489 0.962399 10.3183 1.12369 11.6174Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M1.15527 3.2879L6.49688 7.49958C7.32189 8.15007 8.48531 8.15007 9.31032 7.49958L14.6519 3.2879\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "</svg>\n",
       "<span class=\"text-[#AA5F03]\">Subscribe by Email</span>\n",
       "</div>\n",
       "</a>\n",
       "<div>\n",
       "<a class=\"primary-nav-link !px-0.6\" href=\"/users/sign_in\" title=\"Log in\">\n",
       "<span class=\"link-text\">Log in</span>\n",
       "</a> </div>\n",
       "<span class=\"nav-slash md:mx-1 h-full bg-gradient-to-b from-[#CFCBC9] to-transparent w-[1px]\"></span>\n",
       "<div>\n",
       "<a class=\"primary-nav-link !px-0.6\" href=\"/users/sign_up\" title=\"Sign up\">\n",
       "<span class=\"link-text\">Sign up</span>\n",
       "</a> </div>\n",
       "<a class=\"primary-nav-link !px-0.6\" href=\"https://updates.emergentmind.com/\" title=\"Updates\">\n",
       "<span class=\"link-text\">Updates</span>\n",
       "</a>\n",
       "</div>\n",
       "<!-- Desktop Menu -->\n",
       "<div class=\"h-full w-full hidden lg:flex items-center justify-end space-x-0.6\">\n",
       "<div class=\"h-full flex items-center space-x-2 whitespace-nowrap\">\n",
       "<div>\n",
       "<a class=\"primary-nav-link !px-0.6\" href=\"/users/sign_in\" title=\"Log in\">\n",
       "<span class=\"link-text\">Log in</span>\n",
       "</a> </div>\n",
       "<span class=\"nav-slash md:mx-1 h-full bg-gradient-to-b from-[#CFCBC9] to-transparent w-[1px]\"></span>\n",
       "<div>\n",
       "<a class=\"primary-nav-link !px-0.6\" href=\"/users/sign_up\" title=\"Sign up\">\n",
       "<span class=\"link-text\">Sign up</span>\n",
       "</a> </div>\n",
       "<span class=\"nav-slash md:mx-1 h-full bg-gradient-to-b from-[#CFCBC9] to-transparent w-[1px]\"></span>\n",
       "</div>\n",
       "<div class=\"flex items-center h-full space-x-3.1\">\n",
       "<div class=\"relative pl-2 group\" title=\"Updates\">\n",
       "<svg fill=\"none\" height=\"17\" viewbox=\"0 0 17 17\" width=\"17\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path class=\"group-hover:fill-[#BE8204]\" d=\"M7.96875 0.53125C7.96875 0.239062 8.20781 0 8.5 0C8.79219 0 9.03125 0.239062 9.03125 0.53125V1.08906C11.7174 1.35469 13.8125 3.61914 13.8125 6.375V7.34121C13.8125 8.79219 14.3902 10.1834 15.4162 11.2127L15.5092 11.3057C15.7848 11.5813 15.9408 11.9564 15.9408 12.3449C15.9408 13.1584 15.2834 13.8158 14.4699 13.8158H2.5334C1.71992 13.8125 1.0625 13.1551 1.0625 12.3416C1.0625 11.9531 1.21855 11.5779 1.49414 11.3023L1.58711 11.2094C2.60977 10.1834 3.1875 8.79219 3.1875 7.34121V6.375C3.1875 3.61914 5.28262 1.35469 7.96875 1.08906V0.53125ZM8.5 2.125C6.15254 2.125 4.25 4.02754 4.25 6.375V7.34121C4.25 9.07441 3.5627 10.7379 2.33418 11.9631L2.24453 12.0527C2.16816 12.1291 2.125 12.232 2.125 12.3416C2.125 12.5674 2.30762 12.75 2.5334 12.75H14.4666C14.6924 12.75 14.875 12.5674 14.875 12.3416C14.875 12.232 14.8318 12.1291 14.7555 12.0527L14.6625 11.9598C13.4373 10.7346 12.7467 9.07109 12.7467 7.33789V6.375C12.7467 4.02754 10.8441 2.125 8.49668 2.125H8.5ZM7.49727 15.2303C7.64336 15.642 8.03848 15.9375 8.5 15.9375C8.96152 15.9375 9.35664 15.642 9.50273 15.2303C9.59902 14.9547 9.90449 14.8086 10.1801 14.9049C10.4557 15.0012 10.6018 15.3066 10.5055 15.5822C10.2133 16.409 9.42637 17 8.5 17C7.57363 17 6.78672 16.409 6.49453 15.5822C6.39824 15.3066 6.54102 15.0012 6.81992 14.9049C7.09883 14.8086 7.40098 14.9514 7.49727 15.2303Z\" fill=\"hsl(60,6%,38%)\"></path>\n",
       "</svg>\n",
       "<div class=\"absolute top-[-4px] left-[10px]\">\n",
       "<div class=\"relative\" id=\"headway\"></div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"group h-full\">\n",
       "<a class=\"\" href=\"/subscribe\" title=\"Subscribe by Email\">\n",
       "<div class=\"flex items-center space-x-1.5 group-hover bg-[#FADD07] hover:bg-[#fbe439] pl-4 pr-4.1 h-full rounded-b-xl text-[15px]\">\n",
       "<svg class=\"text-[#AA5F03] -translate-y-0.1\" fill=\"none\" height=\"15\" viewbox=\"0 0 16 15\" width=\"16\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M1.12369 11.6174C1.25877 12.7055 2.13777 13.565 3.22825 13.6791C4.7372 13.8367 6.30109 14.0033 7.90371 14.0033C9.50633 14.0033 11.0702 13.8367 12.5791 13.6791C13.6696 13.565 14.5486 12.7055 14.6837 11.6174C14.845 10.3183 15.0037 8.97489 15.0037 7.59973C15.0037 6.22456 14.845 4.88111 14.6837 3.58205C14.5486 2.49399 13.6696 1.63445 12.5791 1.52045C11.0702 1.36272 9.50633 1.19617 7.90371 1.19617C6.30109 1.19617 4.73719 1.36272 3.22825 1.52045C2.13777 1.63445 1.25877 2.49399 1.12368 3.58205C0.962399 4.88111 0.803711 6.22456 0.803711 7.59973C0.803711 8.97489 0.962399 10.3183 1.12369 11.6174Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M1.15527 3.2879L6.49688 7.49958C7.32189 8.15007 8.48531 8.15007 9.31032 7.49958L14.6519 3.2879\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "</svg>\n",
       "<span class=\"text-[#AA5F03] -translate-y-0.1\">Subscribe</span>\n",
       "</div>\n",
       "</a> </div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</nav>\n",
       "<!-- /end: Main Navigation -->\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"w-full\">\n",
       "<div @keypress.window=\"handleKeyPress($event)\" class=\"mx-auto pt-2.1 sm:pt-2 relative max-w-[800px] w-full\" x-data=\"paper({&quot;id&quot;:387197,&quot;arxiv_paper_id&quot;:&quot;2404.01810&quot;,&quot;title&quot;:&quot;Surface Reconstruction from Gaussian Splatting via Novel Stereo Views&quot;,&quot;paper_url&quot;:&quot;https://www.emergentmind.com/papers/2404.01810&quot;,&quot;published_at&quot;:&quot;2024-04-02T10:13:18.000Z&quot;,&quot;published_at_short&quot;:&quot;Apr  2&quot;,&quot;published_at_long&quot;:&quot;Apr  2, 2024 at 10:13am UTC&quot;,&quot;age_in_seconds&quot;:99634,&quot;categories&quot;:[8],&quot;score&quot;:0.7675061239357331,&quot;status&quot;:&quot;enriched&quot;,&quot;has_bookmarked&quot;:false,&quot;temperature&quot;:100,&quot;figures_count&quot;:15,&quot;twitter_likes_count&quot;:114,&quot;reddit_points_count&quot;:0,&quot;hacker_news_points_count&quot;:0,&quot;youtube_paper_mentions_count&quot;:0,&quot;github_repos_count&quot;:0,&quot;github_pages_count&quot;:0,&quot;github_stars_count&quot;:0,&quot;html_analysis&quot;:&quot;  \\u003cfigure class=\\&quot;py-4\\&quot;\\u003e\\n      \\u003cdiv class=\\&quot;flex flex-col justify-center\\&quot;\\u003e\\n          \\u003cimg class=\\&quot;max-h-[400px]\\&quot; loading=\\&quot;lazy\\&quot; src=\\&quot;https://cdn.filestackcontent.com/resize=w:800,fit:max/auto_image/compress/nwOzXiHhRiWKOBu3CG6t\\&quot; /\\u003e\\n      \\u003c/div\\u003e\\n\\n    \\u003cfigcaption class=\\&quot;text-sm italic text-center text-stone-500 text-balance\\&quot;\\u003e\\n      Pipeline showing steps from 3DGS model scene representation to integrated RGB-D structure using TSDF.\\n    \\u003c/figcaption\\u003e\\n  \\u003c/figure\\u003e\\n\\n  \\u003ch3 class=\\&quot;!border-none\\&quot;\\u003eOverview\\u003c/h3\\u003e\\n  \\n\\u003cul class=\\&quot;\\&quot;\\u003e\\n    \\u003cli\\u003e\\n        \\u003cp\\u003eA new method for surface reconstruction from Gaussian splatting models utilizing novel-view synthesis for stereo image creation and depth information extraction is introduced.\\u003c/p\\u003e\\n    \\u003c/li\\u003e\\n    \\u003cli\\u003e\\n        \\u003cp\\u003eThis approach avoids direct spatial alignment of Gaussian elements, instead generating stereo image pairs, applying stereo matching for depth extraction, and integrating depth data for a unified surface.\\u003c/p\\u003e\\n    \\u003c/li\\u003e\\n    \\u003cli\\u003e\\n        \\u003cp\\u003eThe method has been empirically validated across various scenes, showing superior fidelity and detail in reconstruction over current methods with reduced computational demand.\\u003c/p\\u003e\\n    \\u003c/li\\u003e\\n    \\u003cli\\u003e\\n        \\u003cp\\u003eFuture directions include improving the initial 3DGS capture process and the stereo matching algorithms, particularly for challenging surfaces like transparency.\\u003c/p\\u003e\\n    \\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\n\\n  \\u003ch3 class='paper-heading'\\u003eOverview\\u003c/h3\\u003e\\n\\u003cp\\u003eResearchers have proposed a novel method for \\u003cspan class=\\&quot;border-b whitespace-nowrap\\&quot; x-tooltip.placement.bottom.raw=\\&quot;The process of creating a 3D model from data (e.g., images, depth information), forming a geometrically consistent surface representation of an object or scene.\\&quot;\\u003esurface reconstruction\\u003c/span\\u003e from Gaussian splatting models by exploiting the capacity for high-quality novel-view synthesis inherent in such models. Unlike existing strategies that attempt to align Gaussian elements spatially to build a surface directly, this approach synthesizes stereo image pairs from novel viewpoints and extracts depth information via \\u003cspan class=\\&quot;border-b whitespace-nowrap\\&quot; x-tooltip.placement.bottom.raw=\\&quot;A technique in computer vision where two images taken from slightly different viewpoints are compared to identify corresponding pixels, which can be used to calculate depth information.\\&quot;\\u003estereo matching\\u003c/span\\u003e. This depth data, integrated across views, forms a geometrically consistent surface representation. This methodology not only improves reconstruction fidelity and detail over current methods but does so with significantly reduced computational demand.\\u003c/p\\u003e\\n\\u003ch3 class='paper-heading'\\u003eSurface Reconstruction Challenge in Gaussian Splatting\\u003c/h3\\u003e\\n\\u003cp\\u003eGaussian splatting models (3DGS) optimize a cloud of 3D Gaussians to match input images across varying viewpoints. Despite their effectiveness in novel viewpoint synthesis, leveraging the spatial information of Gaussian elements for direct surface reconstruction presents substantial challenges. The disconnection between the Gaussian elements\\u0026#39; optimized positions for image matching and their utility in forming a coherent surface structure leads to inaccurate and noisy reconstructions.\\u003c/p\\u003e\\n\\u003ch3 class='paper-heading'\\u003eOur Novel Approach\\u003c/h3\\u003e\\n\\u003cp\\u003eThe proposed methodology introduces an innovative pipeline that leverages 3DGS\\u0026#39;s novel-view synthesis in stereo calibration form to generate depth maps, which are then combined to form a refined, geometrically consistent surface mesh. This process involves:\\u003c/p\\u003e\\n\\n\\u003cul\\u003e\\n\\u003cli\\u003e\\u003cp\\u003eCapturing scenes using 3DGS to enable the generation of synthetic stereo image pairs.\\u003c/p\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003cp\\u003eApplying stereo matching algorithms to these image pairs to derive depth information.\\u003c/p\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003cp\\u003eIntegrating collected depth data using Truncated Signed Distance Function (TSDF) techniques to produce a unified and smooth surface mesh.\\u003c/p\\u003e\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\n\\u003cp\\u003eAdditionally, the paper describes a semi-automatic approach for extracting specific objects within scenes for targeted reconstruction by combining depth maps with \\u003cspan class=\\&quot;border-b whitespace-nowrap\\&quot; x-tooltip.placement.bottom.raw=\\&quot;In image processing, a segmentation mask is a binary image that indicates specific parts of an image or scene, such as the location of an object, for further processing or analysis.\\&quot;\\u003esegmentation masks\\u003c/span\\u003e.\\u003c/p\\u003e\\n\\u003ch3 class='paper-heading'\\u003eEmpirical Validation\\u003c/h3\\u003e\\n\\u003cp\\u003eThe effectiveness of the proposed method is confirmed through extensive testing across various scenes, including those captured in uncontrolled, \\u0026quot;in-the-wild\\u0026quot; conditions using standard smartphone cameras. Results on the \\u003cspan class=\\&quot;border-b whitespace-nowrap\\&quot; x-tooltip.placement.bottom.raw=\\&quot;A benchmark suite used in computer vision to evaluate the performance of 3D reconstruction algorithms across various complex scenes.\\&quot;\\u003eTanks and Temples benchmark\\u003c/span\\u003e further validate its superiority in surface reconstruction from Gaussian splatting models over the current leading methods. Notably, the method achieves detailed and accurate reconstructions comparable to the best \\u003cspan class=\\&quot;border-b whitespace-nowrap\\&quot; x-tooltip.placement.bottom.raw=\\&quot;Advanced methods of reconstructing 3D surfaces from data using neural networks, aiming to improve accuracy and detail in the reconstruction process.\\&quot;\\u003eneural surface reconstruction techniques\\u003c/span\\u003e but with a fraction of the computational expense.\\u003c/p\\u003e\\n\\u003ch3 class='paper-heading'\\u003eLimitations and Future Directions\\u003c/h3\\u003e\\n\\u003cp\\u003eWhile the method marks a significant advancement, it is not without limitations. The reconstruction\\u0026#39;s fidelity relies heavily on the initial 3DGS scene capture quality. Inaccuracies in the Gaussian splatting model, particularly in less well-defined regions, can propagate errors into the final surface reconstruction. Additionally, the stereo matching process might struggle with transparent surfaces, potentially leading to incomplete or inaccurate reconstructions in these areas.\\u003c/p\\u003e\\n\\n\\u003cp\\u003eFuture work could explore improving the robustness of the initial 3DGS capture process and enhancing stereo matching algorithms to better handle challenging surface properties like transparency. The fusion of depth data could also benefit from advances in integrating diverse information sources, potentially leading to even more accurate and detailed surface reconstructions.\\u003c/p\\u003e\\n\\u003ch3 class='paper-heading'\\u003eConclusion\\u003c/h3\\u003e\\n\\u003cp\\u003eThis research introduces a significant methodological shift in surface reconstruction from Gaussian splatting models, focusing on leveraging novel-view synthesis for depth extraction rather than direct geometric manipulation of Gaussian elements. This strategy not only surpasses existing methods in accuracy and detail but also significantly reduces computational requirements, representing a meaningful advancement in the field of three-dimensional scene reconstruction.\\u003c/p\\u003e\\n&quot;})\" x-init=\"trackPaperView();\">\n",
       "<div class=\"flex flex-col relative\">\n",
       "<!-- begin: Sidebar -->\n",
       "<div class=\"sidebar container px-2 sm:px-4 mx-auto -mb-4.5 relative sticky -top-[1px] z-30 border-none\" id=\"sidebar\">\n",
       "<div class=\"sidebar-background flex justify-center items-center xs:space-x-3 bg-stone-100/80 backdrop-blur-md border-b border-stone-200\">\n",
       "<div class=\"flex flex-row lg:flex-col space-x-7 relative z-20 pt-1 pb-1\">\n",
       "<!-- begin: Button -->\n",
       "<div class=\"flex items-center space-x-1\">\n",
       "<div class=\"\">\n",
       "<a @click.prevent=\"handleBookmarkClick(false)\" class=\"sidebar-button group\" href=\"/papers/2404.01810\" x-ref=\"bookmark_button\" x-tooltip=\"{\n",
       "            content: () =&gt; 'Or press m',\n",
       "            appendTo: document.querySelector('#sidebar')\n",
       "          }\">\n",
       "<svg class=\"fill-stone-600 group-hover:fill-active-link-color-darker select-none\" fill=\"none\" height=\"24\" viewbox=\"0 0 19 24\" width=\"19\" x-cloak=\"\" x-show=\"paper.has_bookmarked\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M1 3.00682V23.0588L9.355 17.2103L17.71 23.0588V3.00682C17.71 2.08395 16.9619 1.33582 16.039 1.33582H2.671C1.74814 1.33582 1 2.08395 1 3.00682Z\" stroke=\"hsl(33,6%,61%)\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path>\n",
       "</svg>\n",
       "<svg class=\"text-[hsl(33,6%,61%)] group-hover:text-active-link-color-darker select-none\" fill=\"none\" height=\"24\" viewbox=\"0 0 19 24\" width=\"19\" x-show=\"!paper.has_bookmarked\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M1 3.00682V23.0588L9.355 17.2103L17.71 23.0588V3.00682C17.71 2.08395 16.9619 1.33582 16.039 1.33582H2.671C1.74814 1.33582 1 2.08395 1 3.00682Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path>\n",
       "</svg>\n",
       "<span class=\"ml-1\">Bookmark</span>\n",
       "</a> </div>\n",
       "<span class=\"text-[hsl(60,6%,38%)]\">Â·</span>\n",
       "<a @click=\"trackViewTrendingPapersClick()\" class=\"sidebar-button group\" href=\"https://www.emergentmind.com/\">\n",
       "<span class=\"\">Trending Papers</span>\n",
       "<svg class=\"text-[hsl(33,6%,61%)] group-hover:text-active-link-color-darker select-none\" fill=\"none\" height=\"20\" viewbox=\"0 0 19 20\" width=\"19\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M0.521484 10.042H17.5703\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path>\n",
       "<path d=\"M9.56738 19.0926C12.8421 16.369 14.8421 14.5319 17.1983 11.7651C17.6069 11.2856 17.8313 10.6763 17.8313 10.0463C17.8313 9.41634 17.6069 8.80698 17.1983 8.32751C14.8435 5.56073 12.8421 3.72363 9.56738 1\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path>\n",
       "</svg>\n",
       "</a>\n",
       "</div>\n",
       "<!-- /end: Button -->\n",
       "</div>\n",
       "<!-- /end: Sidebar -->\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"pt-5 sm:pt-5 mb-4 container px-2 sm:px-4 mx-auto relative\">\n",
       "<!-- begin: Emergent Mind analysis -->\n",
       "<div class=\"flex space-x-0\">\n",
       "<div class=\"flex chat-bubble white-chat-bubble w-full\">\n",
       "<!-- begin: Avatar -->\n",
       "<div class=\"mr-2.5 -translate-y-1.1 hidden md:inline-block grow-0\">\n",
       "<svg fill=\"none\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<g clip-path=\"url(#clip0_3188_9)\">\n",
       "<path d=\"M0 0H32V32H0V0Z\" fill=\"#F5F5F4\"></path>\n",
       "<path d=\"M16.1328 11.4V8.59998\" stroke=\"hsl(60,6%,38%)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"></path>\n",
       "<path d=\"M16.1332 11.4C14.1882 11.4 12.1572 11.4 10.4312 11.7C9.96415 11.7849 9.53303 12.0072 9.19311 12.3386C8.85318 12.6699 8.61993 13.0953 8.52323 13.56C8.28223 14.676 8.28223 15.596 8.28223 17.471C8.28223 19.346 8.28223 20.267 8.52323 21.383C8.72823 22.333 9.47523 23.075 10.4312 23.242C12.1572 23.544 14.1882 23.544 16.1332 23.544C18.0792 23.544 20.1102 23.544 21.8362 23.242C22.3032 23.1574 22.7343 22.9352 23.0742 22.604C23.4142 22.2728 23.6475 21.8477 23.7442 21.383C23.9842 20.267 23.9842 19.346 23.9842 17.471C23.9842 15.596 23.9842 14.676 23.7442 13.559C23.6473 13.0946 23.4139 12.6696 23.074 12.3386C22.7341 12.0076 22.3031 11.7856 21.8362 11.701C20.1102 11.399 18.0792 11.4 16.1332 11.4ZM16.1332 8.57003C16.4124 8.57612 16.69 8.5264 16.9497 8.42378C17.2094 8.32117 17.4459 8.16771 17.6455 7.97243C17.8451 7.77715 18.0037 7.54397 18.112 7.28658C18.2202 7.02919 18.276 6.75276 18.276 6.47353C18.276 6.19429 18.2202 5.91787 18.112 5.66047C18.0037 5.40308 17.8451 5.1699 17.6455 4.97462C17.4459 4.77934 17.2094 4.62589 16.9497 4.52327C16.69 4.42065 16.4124 4.37093 16.1332 4.37703C15.5771 4.37703 15.0437 4.59796 14.6504 4.99122C14.2572 5.38449 14.0362 5.91787 14.0362 6.47403C14.0362 7.03018 14.2572 7.56357 14.6504 7.95683C15.0437 8.35009 15.5771 8.57103 16.1332 8.57103V8.57003Z\" stroke=\"hsl(60,6%,38%)\" stroke-width=\"1.6\"></path>\n",
       "<path d=\"M13.305 15.8V14.957M18.962 15.8V14.957M13.106 19.357L13.288 19.548C13.579 19.853 13.982 20.025 14.404 20.025H17.864C18.284 20.025 18.687 19.853 18.978 19.548L19.161 19.358\" stroke=\"hsl(60,6%,38%)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.6\"></path>\n",
       "</g>\n",
       "<defs>\n",
       "<clippath id=\"clip0_3188_9\">\n",
       "<rect fill=\"white\" height=\"32\" rx=\"5\" width=\"32\"></rect>\n",
       "</clippath>\n",
       "</defs>\n",
       "</svg>\n",
       "</div>\n",
       "<!-- /end: Avatar -->\n",
       "<!-- begin: Username and content -->\n",
       "<div class=\"\">\n",
       "<div class=\"section-title\">\n",
       "      Emergent Mind\n",
       "    </div>\n",
       "<div class=\"mt-1.5 sm:mt-4 font-sitka\">\n",
       "<a class=\"break-smart\" href=\"https://arxiv.org/abs/2404.01810\" rel=\"nofollow\">\n",
       "<h1 class=\"text-[16px] md:text-[19px] leading-[1.4] md:leading-[1.3] text-[hsl(245,20%,50%)] no-underline font-sitka font-semibold inline\">\n",
       "          Surface Reconstruction from Gaussian Splatting via Novel Stereo Views\n",
       "        </h1>\n",
       "<svg class=\"inline mx-0.5 -translate-y-1 w-2.6 text-[#686490]\" fill=\"none\" viewbox=\"0 0 523 523\"><path d=\"m87.223.199 412.244 23.334 23.334 412.244-87.681 7.071-9.193-200.111c-1.414-32.527-1.414-63.64 4.95-94.046l-374.06 374.06L.25 466.182l374.06-374.06c-34.649 7.779-50.912 7.072-94.046 4.95L80.153 87.88 87.222.2Z\" fill=\"currentColor\"></path></svg>\n",
       "</a>\n",
       "<span class=\"text-[#5B5852] text-[13px] leading-5 font-sitka\">(2404.01810)</span>\n",
       "<div class=\"text-[#5B5852] text-[14px] md:text-[16px] font-sitka mt-2 leading-[1.4]\">\n",
       "        Published Apr  2, 2024\n",
       "            in\n",
       "            <span class=\"\" x-tooltip.raw=\"Computer Vision\">cs.CV</span>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"mt-5 font-sitka\">\n",
       "<div class=\"rich-text font-sitka\">\n",
       "<div class=\"bg-stone-50/80 rounded px-4 md:px-6 py-4 md:-ml-6 md:-mr-1.5\">\n",
       "<h3 class=\"!my-0 !pb-3\">Abstract</h3>\n",
       "<div class=\"\">\n",
       "            The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge. We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements' locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods. We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: <a href=\"https://gs2mesh.github.io/\" rel=\"noopener nofollow\" target=\"_blank\">https://gs2mesh.github.io/</a>.\n",
       "          </div>\n",
       "</div>\n",
       "<figure class=\"py-4\">\n",
       "<div class=\"flex flex-col justify-center\">\n",
       "<img class=\"max-h-[400px]\" loading=\"lazy\" src=\"https://cdn.filestackcontent.com/resize=w:800,fit:max/auto_image/compress/nwOzXiHhRiWKOBu3CG6t\"/>\n",
       "</div>\n",
       "<figcaption class=\"text-sm italic text-center text-stone-500 text-balance\">\n",
       "      Pipeline showing steps from 3DGS model scene representation to integrated RGB-D structure using TSDF.\n",
       "    </figcaption>\n",
       "</figure>\n",
       "<h3 class=\"!border-none\">Overview</h3>\n",
       "<ul class=\"\">\n",
       "<li>\n",
       "<p>A new method for surface reconstruction from Gaussian splatting models utilizing novel-view synthesis for stereo image creation and depth information extraction is introduced.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>This approach avoids direct spatial alignment of Gaussian elements, instead generating stereo image pairs, applying stereo matching for depth extraction, and integrating depth data for a unified surface.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>The method has been empirically validated across various scenes, showing superior fidelity and detail in reconstruction over current methods with reduced computational demand.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>Future directions include improving the initial 3DGS capture process and the stereo matching algorithms, particularly for challenging surfaces like transparency.</p>\n",
       "</li>\n",
       "</ul>\n",
       "<h3 class=\"paper-heading\">Overview</h3>\n",
       "<p>Researchers have proposed a novel method for <span class=\"border-b whitespace-nowrap\" x-tooltip.placement.bottom.raw=\"The process of creating a 3D model from data (e.g., images, depth information), forming a geometrically consistent surface representation of an object or scene.\">surface reconstruction</span> from Gaussian splatting models by exploiting the capacity for high-quality novel-view synthesis inherent in such models. Unlike existing strategies that attempt to align Gaussian elements spatially to build a surface directly, this approach synthesizes stereo image pairs from novel viewpoints and extracts depth information via <span class=\"border-b whitespace-nowrap\" x-tooltip.placement.bottom.raw=\"A technique in computer vision where two images taken from slightly different viewpoints are compared to identify corresponding pixels, which can be used to calculate depth information.\">stereo matching</span>. This depth data, integrated across views, forms a geometrically consistent surface representation. This methodology not only improves reconstruction fidelity and detail over current methods but does so with significantly reduced computational demand.</p>\n",
       "<h3 class=\"paper-heading\">Surface Reconstruction Challenge in Gaussian Splatting</h3>\n",
       "<p>Gaussian splatting models (3DGS) optimize a cloud of 3D Gaussians to match input images across varying viewpoints. Despite their effectiveness in novel viewpoint synthesis, leveraging the spatial information of Gaussian elements for direct surface reconstruction presents substantial challenges. The disconnection between the Gaussian elements' optimized positions for image matching and their utility in forming a coherent surface structure leads to inaccurate and noisy reconstructions.</p>\n",
       "<h3 class=\"paper-heading\">Our Novel Approach</h3>\n",
       "<p>The proposed methodology introduces an innovative pipeline that leverages 3DGS's novel-view synthesis in stereo calibration form to generate depth maps, which are then combined to form a refined, geometrically consistent surface mesh. This process involves:</p>\n",
       "<ul>\n",
       "<li><p>Capturing scenes using 3DGS to enable the generation of synthetic stereo image pairs.</p></li>\n",
       "<li><p>Applying stereo matching algorithms to these image pairs to derive depth information.</p></li>\n",
       "<li><p>Integrating collected depth data using Truncated Signed Distance Function (TSDF) techniques to produce a unified and smooth surface mesh.</p></li>\n",
       "</ul>\n",
       "<p>Additionally, the paper describes a semi-automatic approach for extracting specific objects within scenes for targeted reconstruction by combining depth maps with <span class=\"border-b whitespace-nowrap\" x-tooltip.placement.bottom.raw=\"In image processing, a segmentation mask is a binary image that indicates specific parts of an image or scene, such as the location of an object, for further processing or analysis.\">segmentation masks</span>.</p>\n",
       "<h3 class=\"paper-heading\">Empirical Validation</h3>\n",
       "<p>The effectiveness of the proposed method is confirmed through extensive testing across various scenes, including those captured in uncontrolled, \"in-the-wild\" conditions using standard smartphone cameras. Results on the <span class=\"border-b whitespace-nowrap\" x-tooltip.placement.bottom.raw=\"A benchmark suite used in computer vision to evaluate the performance of 3D reconstruction algorithms across various complex scenes.\">Tanks and Temples benchmark</span> further validate its superiority in surface reconstruction from Gaussian splatting models over the current leading methods. Notably, the method achieves detailed and accurate reconstructions comparable to the best <span class=\"border-b whitespace-nowrap\" x-tooltip.placement.bottom.raw=\"Advanced methods of reconstructing 3D surfaces from data using neural networks, aiming to improve accuracy and detail in the reconstruction process.\">neural surface reconstruction techniques</span> but with a fraction of the computational expense.</p>\n",
       "<h3 class=\"paper-heading\">Limitations and Future Directions</h3>\n",
       "<p>While the method marks a significant advancement, it is not without limitations. The reconstruction's fidelity relies heavily on the initial 3DGS scene capture quality. Inaccuracies in the Gaussian splatting model, particularly in less well-defined regions, can propagate errors into the final surface reconstruction. Additionally, the stereo matching process might struggle with transparent surfaces, potentially leading to incomplete or inaccurate reconstructions in these areas.</p>\n",
       "<p>Future work could explore improving the robustness of the initial 3DGS capture process and enhancing stereo matching algorithms to better handle challenging surface properties like transparency. The fusion of depth data could also benefit from advances in integrating diverse information sources, potentially leading to even more accurate and detailed surface reconstructions.</p>\n",
       "<h3 class=\"paper-heading\">Conclusion</h3>\n",
       "<p>This research introduces a significant methodological shift in surface reconstruction from Gaussian splatting models, focusing on leveraging novel-view synthesis for depth extraction rather than direct geometric manipulation of Gaussian elements. This strategy not only surpasses existing methods in accuracy and detail but also significantly reduces computational requirements, representing a meaningful advancement in the field of three-dimensional scene reconstruction.</p>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<!-- /end: Username and content -->\n",
       "</div>\n",
       "<div class=\"hidden lg:block w-0\">\n",
       "<div class=\"flex relative\">\n",
       "<div class=\"flex flex-col items-center justify-center space-y-2.6 translate-x-3 ml-1.1 mt-3\">\n",
       "<!-- X/Twitter -->\n",
       "<a :href=\"`/papers/${paper.arxiv_paper_id}#x`\" class=\"flex flex-col space-y-1 items-center justify-center group\" x-cloak=\"\" x-show=\"paper.twitter_likes_count &gt; 0\" x-tooltip=\"{\n",
       "        content: () =&gt; $refs.twitter_template.innerHTML,\n",
       "        allowHTML: true,\n",
       "        appendTo: $root\n",
       "      }\">\n",
       "<svg class=\"social-icon-metric unsortable-social-media-icon\" fill=\"none\" height=\"20\" viewbox=\"0 0 20 20\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M3.33301 3.33331L13.1105 16.6666H16.6663L6.88884 3.33331H3.33301Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M3.33301 16.6666L8.97301 11.0266M11.023 8.97665L16.6663 3.33331\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "</svg>\n",
       "<div class=\"social-media-metric-vertical-text\" x-text=\"formatLargeNumber(paper.twitter_likes_count)\"></div>\n",
       "</a>\n",
       "<!-- Reddit -->\n",
       "<a :href=\"`/papers/${paper.arxiv_paper_id}#reddit`\" class=\"flex flex-col space-y-1 items-center justify-center group\" x-cloak=\"\" x-show=\"paper.reddit_points_count &gt; 0\" x-tooltip=\"{\n",
       "        content: () =&gt; $refs.reddit_template.innerHTML,\n",
       "        allowHTML: true,\n",
       "        appendTo: $root\n",
       "      }\">\n",
       "<svg class=\"social-icon-metric unsortable-social-media-icon\" fill=\"none\" height=\"23\" viewbox=\"0 0 23 23\" width=\"23\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M11.5007 7.66669C14.0383 7.66669 16.3192 8.45827 17.8975 9.71752C18.4375 9.52825 19.0273 9.53847 19.5604 9.74632C20.0935 9.95418 20.5346 10.3459 20.8039 10.8508C21.0732 11.3556 21.153 11.9401 21.0287 12.4986C20.9045 13.0572 20.5845 13.5528 20.1266 13.8959C20.1266 17.3363 16.2645 20.125 11.5016 20.125C6.82973 20.125 3.02515 17.4417 2.87661 14.0933L1.91828 13.8959C1.46037 13.5528 1.14036 13.0572 1.01614 12.4986C0.891913 11.9401 0.97167 11.3556 1.24099 10.8508C1.5103 10.3459 1.95139 9.95418 2.48446 9.74632C3.01754 9.53847 3.6074 9.52825 4.14736 9.71752C5.72478 8.45923 8.00561 7.66669 10.5433 7.66669H11.5007Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M11.5 7.66667L12.4583 2.875L18.2083 3.83333\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M17.25 3.83333C17.25 4.0875 17.351 4.33125 17.5307 4.51098C17.7104 4.6907 17.9542 4.79167 18.2083 4.79167C18.4625 4.79167 18.7063 4.6907 18.886 4.51098C19.0657 4.33125 19.1667 4.0875 19.1667 3.83333C19.1667 3.57917 19.0657 3.33541 18.886 3.15569C18.7063 2.97597 18.4625 2.875 18.2083 2.875C17.9542 2.875 17.7104 2.97597 17.5307 3.15569C17.351 3.33541 17.25 3.57917 17.25 3.83333Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M8.62565 12.9375C8.89029 12.9375 9.10482 12.723 9.10482 12.4584C9.10482 12.1937 8.89029 11.9792 8.62565 11.9792C8.36101 11.9792 8.14648 12.1937 8.14648 12.4584C8.14648 12.723 8.36101 12.9375 8.62565 12.9375Z\" fill=\"#8F8F80\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M14.3757 12.9375C14.6403 12.9375 14.8548 12.723 14.8548 12.4584C14.8548 12.1937 14.6403 11.9792 14.3757 11.9792C14.111 11.9792 13.8965 12.1937 13.8965 12.4584C13.8965 12.723 14.111 12.9375 14.3757 12.9375Z\" fill=\"#8F8F80\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M9.58398 16.2917C10.2232 16.6108 10.8614 16.7709 11.5007 16.7709C12.1399 16.7709 12.7781 16.6108 13.4173 16.2917\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "</svg>\n",
       "<div class=\"social-media-metric-vertical-text\" x-text=\"formatLargeNumber(paper.reddit_points_count)\"></div>\n",
       "</a>\n",
       "<!-- HackerNews -->\n",
       "<a :href=\"`/papers/${paper.arxiv_paper_id}#hackernews`\" class=\"flex flex-col space-y-1 items-center justify-center group\" x-cloak=\"\" x-show=\"paper.hacker_news_points_count &gt; 0\" x-tooltip=\"{\n",
       "        content: () =&gt; $refs.hackernews_template.innerHTML,\n",
       "        allowHTML: true,\n",
       "        appendTo: $root\n",
       "      }\">\n",
       "<svg class=\"social-icon-metric translate-y-[1px] unsortable-social-media-icon\" fill=\"none\" height=\"23\" viewbox=\"0 0 23 23\" width=\"23\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M3.83398 5.75004C3.83398 5.24171 4.03592 4.7542 4.39536 4.39475C4.75481 4.03531 5.24232 3.83337 5.75065 3.83337L17.2507 3.83337C17.759 3.83337 18.2465 4.03531 18.6059 4.39475C18.9654 4.7542 19.1673 5.24171 19.1673 5.75004V17.25C19.1673 17.7584 18.9654 18.2459 18.6059 18.6053C18.2465 18.9648 17.759 19.1667 17.2507 19.1667H5.75065C5.24232 19.1667 4.75481 18.9648 4.39536 18.6053C4.03592 18.2459 3.83398 17.7584 3.83398 17.25L3.83398 5.75004Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M7.66602 6.70831L11.4993 12.4583L15.3327 6.70831\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M11.5 16.2916V12.4583\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "</svg>\n",
       "<div class=\"social-media-metric-vertical-text\" x-text=\"formatLargeNumber(paper.hacker_news_points_count)\"></div>\n",
       "</a>\n",
       "<!-- YouTube -->\n",
       "<a :href=\"`/papers/${paper.arxiv_paper_id}#youtube`\" class=\"flex flex-col space-y-1 items-center justify-center group\" x-cloak=\"\" x-show=\"paper.youtube_paper_mentions_count &gt; 0\" x-tooltip=\"{\n",
       "        content: () =&gt; $refs.youtube_template.innerHTML,\n",
       "        allowHTML: true,\n",
       "        appendTo: $root\n",
       "      }\">\n",
       "<svg class=\"social-icon-metric translate-y-0.1 unsortable-social-media-icon\" fill=\"none\" height=\"23\" viewbox=\"0 0 24 23\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M1.99707 7.66671C1.99707 6.65004 2.41777 5.67502 3.16661 4.95613C3.91545 4.23724 4.9311 3.83337 5.99013 3.83337H17.9693C19.0283 3.83337 20.044 4.23724 20.7928 4.95613C21.5417 5.67502 21.9623 6.65004 21.9623 7.66671V15.3334C21.9623 16.35 21.5417 17.3251 20.7928 18.0439C20.044 18.7628 19.0283 19.1667 17.9693 19.1667H5.99013C4.9311 19.1667 3.91545 18.7628 3.16661 18.0439C2.41777 17.3251 1.99707 16.35 1.99707 15.3334V7.66671Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "<path d=\"M9.98242 8.625L14.9737 11.5L9.98242 14.375V8.625Z\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "</svg>\n",
       "<div class=\"social-media-metric-vertical-text\" x-text=\"formatLargeNumber(paper.youtube_paper_mentions_count)\"></div>\n",
       "</a>\n",
       "<!-- GitHub -->\n",
       "<a :href=\"`/papers/${paper.arxiv_paper_id}#github`\" class=\"flex flex-col space-y-1 items-center justify-center group\" x-cloak=\"\" x-show=\"paper.github_repos_count &gt; 0 || paper.github_pages_count &gt; 0\" x-tooltip=\"{\n",
       "        content: () =&gt; $refs.github_template.innerHTML,\n",
       "        allowHTML: true,\n",
       "        appendTo: $root\n",
       "      }\">\n",
       "<svg class=\"social-icon-metric translate-y-0 unsortable-social-media-icon\" fill=\"none\" height=\"22\" viewbox=\"0 0 22 22\" width=\"22\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M8.25 17.4167C4.30833 18.7 4.30833 15.125 2.75 14.6667M13.75 19.25V16.0417C13.75 15.125 13.8417 14.7583 13.2917 14.2083C15.8583 13.9333 18.3333 12.925 18.3333 8.70835C18.3322 7.6129 17.9048 6.56088 17.1417 5.77502C17.4996 4.82349 17.4666 3.76901 17.05 2.84168C17.05 2.84168 16.0417 2.56668 13.8417 4.03335C11.9783 3.54805 10.0217 3.54805 8.15833 4.03335C5.95833 2.56668 4.95 2.84168 4.95 2.84168C4.53336 3.76901 4.50041 4.82349 4.85833 5.77502C4.09517 6.56088 3.66778 7.6129 3.66667 8.70835C3.66667 12.925 6.14167 13.9333 8.70833 14.2083C8.15833 14.7583 8.15833 15.3083 8.25 16.0417V19.25\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.25\"></path>\n",
       "</svg>\n",
       "<div class=\"social-media-metric-vertical-text\" x-text=\"paper.github_repos_count &gt; 0 ? formatLargeNumber(paper.github_stars_count) : paper.github_pages_count\"></div>\n",
       "</a>\n",
       "</div>\n",
       "<template x-ref=\"twitter_template\">\n",
       "<div x-text=\"pluralize(paper.twitter_likes_count, 'like on X', 'likes on X')\"></div>\n",
       "</template>\n",
       "<template x-ref=\"hackernews_template\">\n",
       "<div x-text=\"pluralize(paper.hacker_news_points_count, 'upvote on HackerNews', 'upvotes on HackerNews')\"></div>\n",
       "</template>\n",
       "<template x-ref=\"reddit_template\">\n",
       "<div x-text=\"pluralize(paper.reddit_points_count, 'point on Reddit', 'points on Reddit')\"></div>\n",
       "</template>\n",
       "<template x-ref=\"youtube_template\">\n",
       "<div x-text=\"pluralize(paper.youtube_paper_mentions_count, 'video on YouTube', 'video on YouTube')\"></div>\n",
       "</template>\n",
       "<template x-ref=\"github_template\">\n",
       "<div x-show=\"paper.github_repos_count &gt; 0 &amp;&amp; paper.github_pages_count == 0\" x-text=\"pluralize(paper.github_stars_count, 'star on GitHub', 'stars on GitHub')\"></div>\n",
       "<div x-show=\"paper.github_repos_count == 0 &amp;&amp; paper.github_pages_count &gt; 0\" x-text=\"pluralize(paper.github_pages_count, 'GitHub page', 'GitHub page')\"></div>\n",
       "<div x-show=\"paper.github_repos_count &gt; 0 &amp;&amp; paper.github_pages_count &gt; 0\" x-text=\"`${pluralize(paper.github_stars_count, 'star', 'stars')} and ${pluralize(paper.github_pages_count, 'GitHub page', 'GitHub pages')}`\"></div>\n",
       "</template>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<!-- /end: Emergent Mind analysis -->\n",
       "<div class=\"mt-12 max-w-xl mx-auto\">\n",
       "<h2 class=\"text-2xl text-stone-700 font-semibold text-center text-balance\">Get summaries of trending AI papers delivered straight to your inbox</h2>\n",
       "<form accept-charset=\"UTF-8\" action=\"/subscribers\" class=\"mx-auto mt-6 flex space-x-1 items-center\" method=\"post\"><input autocomplete=\"off\" name=\"authenticity_token\" type=\"hidden\" value=\"4gzf2gj_MfX8x6G185W92bmIGlX_ahWmlTlo-e6wuVca9VAE8XdMExZXjKVgB34VNIzxOS9U9N2sqWXDmmR1nw\">\n",
       "<div class=\"flex flex-col sm:flex-row w-full space-y-2 sm:space-x-1 sm:space-y-0 justify-center items-center\">\n",
       "<div class=\"flex space-x-1\">\n",
       "<input class=\"w-[210px] sm:w-[260px] rounded-md border-0 p-2 text-gray-900 shadow-sm ring-1 ring-inset ring-gray-300 placeholder:text-gray-400 focus:ring-2 focus:ring-inset focus:ring-indigo-600\" data-1p-ignore=\"true\" id=\"subscriber_email\" name=\"subscriber[email]\" placeholder=\"Email address\" required=\"required\" type=\"text\"/>\n",
       "<select class=\"w-[130px] rounded-md border-0 py-1.5 pl-3 pr-10 text-gray-900 ring-1 ring-inset ring-gray-300 focus:ring-2 focus:ring-indigo-600 sm:text-sm sm:leading-6 h-full !py-2\" id=\"subscriber_frequency\" name=\"subscriber[frequency]\"><option selected=\"selected\" value=\"0\">Daily</option>\n",
       "<option value=\"1\">Weekly</option></select>\n",
       "<input autocomplete=\"off\" id=\"subscriber_source\" name=\"subscriber[source]\" type=\"hidden\" value=\"paper_page\"/>\n",
       "<div class=\"hidden\">\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_2\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"2\"/>\n",
       "<label for=\"subscriber_category_2\">Artificial Intelligence</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_6\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"6\"/>\n",
       "<label for=\"subscriber_category_6\">Computation and Language</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_8\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"8\"/>\n",
       "<label for=\"subscriber_category_8\">Computer Vision</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_9\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"9\"/>\n",
       "<label for=\"subscriber_category_9\">Computers and Society</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_11\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"11\"/>\n",
       "<label for=\"subscriber_category_11\">Distributed Computing</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_15\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"15\"/>\n",
       "<label for=\"subscriber_category_15\">Emerging Technologies</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_20\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"20\"/>\n",
       "<label for=\"subscriber_category_20\">Human-Computer Interaction</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_21\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"21\"/>\n",
       "<label for=\"subscriber_category_21\">Information Retrieval</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_23\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"23\"/>\n",
       "<label for=\"subscriber_category_23\">Machine Learning</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_25\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"25\"/>\n",
       "<label for=\"subscriber_category_25\">Multiagent Systems</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_26\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"26\"/>\n",
       "<label for=\"subscriber_category_26\">Multimedia</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_29\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"29\"/>\n",
       "<label for=\"subscriber_category_29\">Neural/Evolutionary Computing</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_35\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"35\"/>\n",
       "<label for=\"subscriber_category_35\">Robotics</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_37\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"37\"/>\n",
       "<label for=\"subscriber_category_37\">Sound</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_44\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"44\"/>\n",
       "<label for=\"subscriber_category_44\">Audio and Speech Processing</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_45\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"45\"/>\n",
       "<label for=\"subscriber_category_45\">Image and Video Processing</label>\n",
       "</div>\n",
       "<div>\n",
       "<input checked=\"checked\" class=\"form-checkbox\" id=\"subscriber_category_153\" name=\"subscriber[category_ids][]\" type=\"checkbox\" value=\"153\"/>\n",
       "<label for=\"subscriber_category_153\">Machine Learning</label>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div>\n",
       "<input class=\"bg-[#FADD07] hover:bg-[#fbe439] py-2 px-6 rounded-lg text-[#AA5F03] cursor-pointer\" data-disable-with=\"Subscribe\" name=\"commit\" type=\"submit\" value=\"Subscribe\"/>\n",
       "</div>\n",
       "</div>\n",
       "</input></form>\n",
       "<p class=\"text-center text-sm text-stone-600 mt-2 sm:mt-4\">Unsubscribe anytime.</p>\n",
       "</div>\n",
       "<!-- begin: Tweets -->\n",
       "<div class=\"pt-7.5\" id=\"x\">\n",
       "<div class=\"flex chat-bubble white-chat-bubble w-full\">\n",
       "<!-- begin: Avatar -->\n",
       "<div class=\"mr-2.5 -translate-y-1.1 hidden md:inline-block grow-0\">\n",
       "<svg class=\"w-[32px] h-[32px] rounded\" style=\"enable-background:new 0 0 24 24;\" version=\"1.1\" viewbox=\"0 0 24 24\" xml:space=\"preserve\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><g><polygon points=\"12.153992,10.729553 8.088684,5.041199 5.92041,5.041199 10.956299,12.087097 11.59021,12.97345    15.900635,19.009583 18.068909,19.009583 12.785217,11.615906  \"></polygon><path d=\"M21.15979,1H2.84021C1.823853,1,1,1.823853,1,2.84021v18.31958C1,22.176147,1.823853,23,2.84021,23h18.31958   C22.176147,23,23,22.176147,23,21.15979V2.84021C23,1.823853,22.176147,1,21.15979,1z M15.235352,20l-4.362549-6.213013   L5.411438,20H4l6.246887-7.104675L4,4h4.764648l4.130127,5.881958L18.06958,4h1.411377l-5.95697,6.775635L20,20H15.235352z\"></path></g></svg>\n",
       "</div>\n",
       "<!-- /end: Avatar -->\n",
       "<!-- begin: Username and content -->\n",
       "<div class=\"w-full\">\n",
       "<div class=\"section-title\">\n",
       "                X\n",
       "              </div>\n",
       "<div class=\"mt-1.5 sm:mt-4 rich-text font-sitka w-full\">\n",
       "<div class=\"flex flex-col space-y-4\">\n",
       "<div class=\"\">\n",
       "<blockquote class=\"twitter-tweet\">\n",
       "<a href=\"https://twitter.com/janusch_patas/status/1775382337406095634\">https://twitter.com/janusch_patas/status/1775382337406095634</a>\n",
       "</blockquote>\n",
       "</div>\n",
       "<div class=\"\">\n",
       "<blockquote class=\"twitter-tweet\">\n",
       "<a href=\"https://twitter.com/zhenjun_zhao/status/1775415943700905992\">https://twitter.com/zhenjun_zhao/status/1775415943700905992</a>\n",
       "</blockquote>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<!-- /end: Username and content -->\n",
       "</div>\n",
       "</div>\n",
       "<!-- /end: Tweets -->\n",
       "<!-- begin: Related Papers -->\n",
       "<div class=\"pt-7.5\" id=\"related-papers\">\n",
       "<div class=\"flex chat-bubble white-chat-bubble w-full\">\n",
       "<!-- begin: Avatar -->\n",
       "<div class=\"mr-2.5 hidden md:inline-block grow-0\">\n",
       "<svg class=\"w-[24px] h-[24px] rounded fill-stone-700\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M48 416v48h64V416H48zm88 89.6c-7.1 4.1-15.3 6.4-24 6.4H48c-26.5 0-48-21.5-48-48V416 392 368 144 120 96 48C0 21.5 21.5 0 48 0h64c8.7 0 16.9 2.3 24 6.4C143.1 2.3 151.3 0 160 0h64c20.6 0 38.1 12.9 45 31.1c5.6-6.1 12.9-10.7 21.4-13L349.9 1.6c24.7-6.8 50.1 8.3 56.7 33.8l18 69.2 6 23.2 61.8 238.3 6 23.2 11.9 46c6.6 25.5-8 51.7-32.7 58.5l-59.6 16.5c-24.7 6.8-50.1-8.3-56.7-33.8l-18-69.2-6-23.2L275.6 145.9 272 132.2V144 368v24 24 48c0 26.5-21.5 48-48 48H160c-8.7 0-16.9-2.3-24-6.4zM160 464h64V416H160v48zM112 48H48V96h64V48zm0 96H48V368h64V144zm48-48h64V48H160V96zm64 272V144H160V368h64zm216.1-12.3l-55.8-215-56.5 15.6 55.8 215 56.5-15.6zm-44.4 62.1l11.9 45.7L464 447.9c0-.1 0-.2 0-.3l0-.1-11.7-45.2-56.5 15.6zm-79.9-308l56.5-15.6L360.4 48.5 304 64.1c0 .1 0 .2 0 .4l11.7 45.2z\"></path></svg>\n",
       "</div>\n",
       "<!-- /end: Avatar -->\n",
       "<!-- begin: Username and content -->\n",
       "<div class=\"\">\n",
       "<div class=\"section-title\">\n",
       "                Related Papers\n",
       "              </div>\n",
       "<div class=\"mt-1.5 sm:mt-4 rich-text font-sitka\">\n",
       "<ol>\n",
       "<li>\n",
       "<a href=\"https://www.emergentmind.com/papers/2403.17888\">2D Gaussian Splatting for Geometrically Accurate Radiance Fields</a>\n",
       "</li>\n",
       "<li>\n",
       "<a href=\"https://www.emergentmind.com/papers/2311.16493\">Mip-Splatting: Alias-free 3D Gaussian Splatting</a>\n",
       "</li>\n",
       "<li>\n",
       "<a href=\"https://www.emergentmind.com/papers/2403.11324\">GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering</a>\n",
       "</li>\n",
       "<li>\n",
       "<a href=\"https://www.emergentmind.com/papers/2403.14627\">MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</a>\n",
       "</li>\n",
       "<li>\n",
       "<a href=\"https://www.emergentmind.com/papers/2403.19495\">CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</a>\n",
       "</li>\n",
       "</ol>\n",
       "</div>\n",
       "</div>\n",
       "<!-- /end: Username and content -->\n",
       "</div>\n",
       "</div>\n",
       "<!-- /end: Tweets -->\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<footer class=\"text-center pt-4 sm:pt-8 pb-3 sm:pb-6 text-base\">\n",
       "<div class=\"flex items-center justify-center text-stone-600 font-verdana text-sm space-x-1 mb-4\">\n",
       "<a class=\"twitter-follow-button\" data-show-count=\"false\" href=\"https://twitter.com/EmergentMind\">Follow @EmergentMind</a>\n",
       "<div class=\"hidden sm:block\">for summaries of trending AI papers</div>\n",
       "</div>\n",
       "<ul class=\"text-gray-600 mb-4 sm:mb-3 flex flex-col sm:flex-row items-center justify-center\">\n",
       "<div>\n",
       "<a class=\"px-2 py-2.5 inline-block hover:text-active-link-color-darker transition duration-75 ease-out\" href=\"https://www.emergentmind.com/about\">About</a>\n",
       "<span class=\"text-section-line-gray\">|</span>\n",
       "<a class=\"px-2 py-2.5 inline-block hover:text-active-link-color-darker transition duration-75 ease-out\" href=\"https://www.emergentmind.com/hi\">Let's Chat</a>\n",
       "<span class=\"text-section-line-gray\">|</span>\n",
       "<a class=\"px-2 py-2.5 inline-block hover:text-active-link-color-darker transition duration-75 ease-out\" href=\"https://www.emergentmind.com/terms\">Terms</a>\n",
       "</div>\n",
       "<div>\n",
       "<span class=\"text-section-line-gray hidden sm:inline\">|</span>\n",
       "<a class=\"px-2 py-2.5 inline-block hover:text-active-link-color-darker transition duration-75 ease-out\" href=\"https://www.emergentmind.com/privacy\">Privacy</a>\n",
       "<span class=\"text-section-line-gray\">|</span>\n",
       "<a class=\"px-2 py-2.5 inline-block hover:text-active-link-color-darker transition duration-75 ease-out\" href=\"https://www.emergentmind.com/feeds/rss\">RSS</a>\n",
       "<span class=\"text-section-line-gray\">|</span>\n",
       "<a class=\"px-2 py-2.5 inline-block hover:text-active-link-color-darker transition duration-75 ease-out\" href=\"https://www.emergentmind.com/contact\">Contact</a>\n",
       "</div>\n",
       "</ul>\n",
       "</footer>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the URL into a BeautifulSoup object\n",
    "response = requests.get(args.url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Surface Reconstruction from Gaussian Splatting via Novel Stereo Views'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the title from soup\n",
    "\n",
    "\"\"\"\n",
    "<h1 class=\"text-[16px] md:text-[19px] leading-[1.4] md:leading-[1.3] text-[hsl(245,20%,50%)] no-underline font-sitka font-semibold inline\">\n",
    "          The Era of Semantic Decoding\n",
    "        </h1>\n",
    "\"\"\"\n",
    "\n",
    "title = soup.find(\"h1\").text.strip()\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Arxiv ID\n",
    "# Iterate through spans until we regex match the arxiv ID structure\n",
    "\n",
    "\"\"\"\n",
    "<span class=\"text-[#5B5852] text-[13px] leading-5 font-sitka\">(2403.14562)</span>\n",
    "\"\"\"\n",
    "\n",
    "arxiv_id = None\n",
    "for span in soup.find_all(\"span\"):\n",
    "    match = re.match(r\"\\((\\d{4}\\.\\d{5})\\)\", span.text)\n",
    "    if match:\n",
    "        arxiv_id = match.group(1)\n",
    "        break\n",
    "    \n",
    "arxiv_id\n",
    "\n",
    "# We can also construct the URLs from the arxiv ID\n",
    "arxiv_url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "em_url = f\"https://www.emergentmind.com/papers/{arxiv_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-04-02'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the publication date\n",
    "\n",
    "\"\"\"\n",
    "<div class=\"text-[#5B5852] text-[14px] md:text-[16px] font-sitka mt-2 leading-[1.4]\">\n",
    "        Published Mar 21, 2024\n",
    "            in\n",
    "            <span class=\"\" x-tooltip.raw=\"Computation and Language\">cs.CL</span>\n",
    "                <span class=\"-ml-[3px]\">,</span>\n",
    "            <span class=\"\" x-tooltip.raw=\"Artificial Intelligence\">cs.AI</span>\n",
    "                <span class=\"-ml-[3px]\">,</span>\n",
    "            <span class=\"\" x-tooltip.raw=\"Human-Computer Interaction\">cs.HC</span>\n",
    "                <span class=\"-ml-[3px]\">,</span>\n",
    "            <span class=\"\" x-tooltip.raw=\"Multiagent Systems\">cs.MA</span>\n",
    "                and\n",
    "\n",
    "          by <a class=\"border-b border-gray-100 hover:text-stone-500\" href=\"https://www.emergentmind.com/search?q=Maxime+Peyrard\">Maxime Peyrard</a>, <a class=\"border-b border-gray-100 hover:text-stone-500\" href=\"https://www.emergentmind.com/search?q=Martin+Josifoski\">Martin Josifoski</a>, and <a class=\"border-b border-gray-100 hover:text-stone-500\" href=\"https://www.emergentmind.com/search?q=Robert+West\">Robert West</a>.\n",
    "      </div>\n",
    "\"\"\"\n",
    "\n",
    "# For the date, we can look for the text \"Published\" and extract the date\n",
    "date = None\n",
    "for div in soup.find_all(\"div\"):\n",
    "    if \"Published\" in div.text:\n",
    "        match = re.search(r\"Published (\\w+\\s+\\d{1,2}, \\d{4})\", div.text)\n",
    "        if match:\n",
    "            date = match.group(1)\n",
    "        break\n",
    "    \n",
    "# This comes out like 'Mar 21, 2024', so we can parse it with datetime\n",
    "try:\n",
    "    date = datetime.datetime.strptime(date, \"%b %d, %Y\").date()\n",
    "    date = date.strftime(\"%Y-%m-%d\")\n",
    "except:\n",
    "    print(\"Could not parse date\")\n",
    "    \n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the abstract, find the H3 tag with the text \"Abstract\"\n",
    "# The next sibling is the actual abstract\n",
    "\n",
    "abstract = None\n",
    "for h3 in soup.find_all(\"h3\"):\n",
    "    if h3.text.strip() == \"Abstract\":\n",
    "        abstract = h3.find_next_sibling(\"div\").text.strip()\n",
    "        break\n",
    "    \n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to dataframe\n",
    "new_row = pd.DataFrame(\n",
    "    {\n",
    "        \"arxiv_id\": [arxiv_id],\n",
    "        \"title\": [title],\n",
    "        \"publication_date\": [date],\n",
    "        \"abstract\": [abstract],\n",
    "        \"notes\": [args.notes],\n",
    "        \"arxiv_url\": [arxiv_url],\n",
    "        \"em_url\": [em_url],\n",
    "    }\n",
    ")\n",
    "\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Save the dataframe\n",
    "df.to_excel(f\"{GOOGLE_DRIVE_PATH}/arxiv_papers.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
